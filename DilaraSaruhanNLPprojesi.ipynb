{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PkkX3C1E90vk5votJa-6MaD2jbwmS51K",
      "authorship_tag": "ABX9TyOecuVBip3TwCQfqQ/lTCmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DilaraSaruhan/Natural-Language-Processing/blob/main/DilaraSaruhanNLPprojesi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Doğal Dil İşleme Dersi - BIL570**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Dilara Saruhan** ⭐\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZThBTpvxyQzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DÖNEM PROJESİ**\n",
        "\n",
        "**KONU:** Duygu Analizi İçin Sözlük Ve Model Oluşturma\n",
        "\n",
        "▶ Linki aşağıda verilen sinema duygu analiz (pozitif, negatif ve nötür)  veri setini kullanarak 3 adet (pozitif, negatif ve nötür) her biri 50 kelimeden oluşan duygu sözlükleri oluşturmanız beklenmektedir (istediğiniz yöntemi kullanabilirsiniz).\n",
        "\n",
        "Veri seti eğitim ve sınama için ikiye ayrılmıştır;\n",
        "\n",
        " 1)Sözlüklerin eğitim veri seti kullanılarak oluşturulması,\n",
        "\n",
        " 2)Elde edilen sözlükler kullanılarak sınama verilerinin duygularını maksimum tespit edecek bir model önerilmesi beklenmektedir.\n",
        "\n",
        "https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "C69SzctHzMHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'ı '/content/drive' dizinine bağladım\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Dosya yolu\n",
        "file_path = '/content/drive/MyDrive/Colab_Datasets/train/'\n",
        "\n",
        "texts_and_labels = []  # Metinleri ve etiketlerini tutacak listeyi oluşturdum\n",
        "\n",
        "# Dosyaları okuyup listede tutuyorum\n",
        "for label in [\"pos\", \"neg\", \"unsup\"]:\n",
        "    path = os.path.join(file_path, label)\n",
        "    for file in os.listdir(path):\n",
        "        file_full_path = os.path.join(path, file)\n",
        "        try:\n",
        "            with open(file_full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                texts_and_labels.append((text, label))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_full_path}: {e}\")\n",
        "\n",
        "# Temizleme işleri için koşullarım\n",
        "stop_words_and_punctuation = set(stopwords.words('english') + list(punctuation))\n",
        "pos_words = []\n",
        "neg_words = []\n",
        "neu_words = []\n",
        "\n",
        "for text, label in texts_and_labels:\n",
        "    text = re.sub('<.*?>', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [w for w in words if not w in stop_words_and_punctuation]\n",
        "    pos = nltk.pos_tag(words)\n",
        "    words = [p[0].lower() for p in pos if p[1][0] == \"J\"]\n",
        "    if label == \"pos\":\n",
        "        pos_words.extend(words)\n",
        "    elif label == \"neg\":\n",
        "        neg_words.extend(words)\n",
        "    else:\n",
        "        neu_words.extend(words)\n",
        "\n",
        "# Her bir kelime listesi için frekans dağılımı oluşturdum\n",
        "pos_freq = nltk.FreqDist(pos_words)\n",
        "neg_freq = nltk.FreqDist(neg_words)\n",
        "neu_freq = nltk.FreqDist(neu_words)\n",
        "\n",
        "# Her bir frekans dağılımından en sık geçen 50 kelimeyi seçtim\n",
        "pos_dict = dict(pos_freq.most_common(50))\n",
        "neg_dict = dict(neg_freq.most_common(50))\n",
        "neu_dict = dict(neu_freq.most_common(50))\n",
        "\n",
        "# Sözlükleri ekrana yazdırdım\n",
        "print(\"Pozitif Duygu Sözlüğü:\")\n",
        "print(pos_dict)\n",
        "\n",
        "print(\"\\nNegatif Duygu Sözlüğü:\")\n",
        "print(neg_dict)\n",
        "\n",
        "print(\"\\nNötr Duygu Sözlüğü:\")\n",
        "print(neu_dict)\n",
        "\n",
        "\n",
        "# Eğitim verilerini oluşturdum\n",
        "X = [text for text, label in texts_and_labels]\n",
        "y = [label for text, label in texts_and_labels]\n",
        "\n",
        "# TF-IDF vektörleme işlemi yaptım\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Veriyi eğitim ve test setlerine böldüm\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Naive Bayes sınıflandırma modelini eğittim\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Modelin performansını değerlendirdim\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Sınama verileri üzerinde modelin performansını gösterdim\n",
        "test_file_path = '/content/drive/MyDrive/Colab_Datasets/test/'\n",
        "\n",
        "test_texts_and_labels = []\n",
        "for label in [\"pos\", \"neg\", \"unsup\"]:\n",
        "    path = os.path.join(test_file_path, label)\n",
        "    for file in os.listdir(path):\n",
        "        with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "            test_texts_and_labels.append((text, label))\n",
        "\n",
        "X_test_data = [text for text, label in test_texts_and_labels]\n",
        "y_test_data = [label for text, label in test_texts_and_labels]\n",
        "\n",
        "X_test_tfidf = vectorizer.transform(X_test_data)\n",
        "\n",
        "# Modelin performansını değerlendirdim\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "accuracy_test = accuracy_score(y_test_data, y_test_pred)\n",
        "classification_report_test = classification_report(y_test_data, y_test_pred)\n",
        "\n",
        "print(f\"\\nModel Accuracy on Test Data: {accuracy_test}\")\n",
        "print(f\"\\nClassification Report on Test Data:\\n{classification_report_test}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VorF1biVKcB",
        "outputId": "9dbc48e9-df44-4c75-8421-236c12f7c725"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pozitif Duygu Sözlüğü:\n",
            "{'good': 7027, 'great': 5872, 'many': 3643, 'best': 3252, 'much': 3133, 'little': 2954, 'first': 2564, 'real': 2364, 'young': 2088, 'old': 1941, 'bad': 1753, 'new': 1674, 'last': 1501, 'big': 1469, 'different': 1464, 'funny': 1367, 'true': 1343, 'original': 1304, 'excellent': 1271, 'beautiful': 1255, 'whole': 1204, 'wonderful': 1137, 'main': 1009, 'classic': 1007, 'special': 947, 'nice': 945, 'small': 923, 'least': 917, 'sure': 912, 'hard': 907, 'right': 901, 'short': 880, 'better': 878, 'second': 859, 'human': 854, 'american': 848, 'high': 801, 'full': 789, 'long': 773, 'several': 761, 'perfect': 753, 'interesting': 740, 'early': 724, 'able': 713, 'strong': 702, 'final': 688, 'fine': 688, 'black': 679, 'favorite': 669, 'enough': 666}\n",
            "\n",
            "Negatif Duygu Sözlüğü:\n",
            "{'bad': 6762, 'good': 6708, 'much': 3308, 'many': 2826, 'little': 2757, 'great': 2469, 'first': 2208, 'real': 1995, 'old': 1902, 'original': 1867, 'worst': 1794, 'least': 1691, 'whole': 1656, 'best': 1583, 'funny': 1569, 'big': 1407, 'last': 1353, 'poor': 1346, 'new': 1332, 'terrible': 1244, 'better': 1229, 'main': 1201, 'stupid': 1182, 'awful': 1165, 'young': 1155, 'sure': 1087, 'special': 1027, 'hard': 997, 'worse': 929, 'horrible': 911, 'wrong': 874, 'different': 860, 'entire': 853, 'right': 833, 'low': 827, 'interesting': 803, 'enough': 802, 'high': 791, 'second': 770, 'true': 744, 'short': 738, 'ridiculous': 723, 'long': 723, 'black': 721, 'nice': 718, 'dead': 715, 'full': 652, 'next': 643, 'obvious': 633, 'several': 622}\n",
            "\n",
            "Nötr Duygu Sözlüğü:\n",
            "{'good': 6770, 'great': 5024, 'many': 3444, 'much': 3065, 'bad': 3021, 'best': 2849, 'little': 2841, 'first': 2437, 'real': 2393, 'young': 1792, 'old': 1729, 'last': 1593, 'funny': 1559, 'new': 1554, 'different': 1422, 'big': 1358, 'whole': 1325, 'original': 1218, 'true': 1178, 'least': 1109, 'special': 1028, 'main': 1027, 'excellent': 1020, 'better': 1007, 'american': 924, 'wonderful': 916, 'beautiful': 902, 'right': 901, 'sure': 870, 'second': 858, 'hard': 853, 'classic': 846, 'high': 837, 'long': 830, 'interesting': 829, 'small': 822, 'nice': 797, 'short': 785, 'full': 772, 'several': 770, 'early': 707, 'enough': 702, 'able': 698, 'human': 688, 'poor': 674, 'next': 671, 'black': 639, 'worth': 631, 'wrong': 629, 'entire': 626}\n",
            "Model Accuracy: 0.5617333333333333\n",
            "\n",
            "Model Accuracy on Test Data: 0.54464\n",
            "\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.67      0.76      0.71     12500\n",
            "         pos       0.52      0.61      0.56     12500\n",
            "       unsup       0.38      0.26      0.31     12500\n",
            "\n",
            "    accuracy                           0.54     37500\n",
            "   macro avg       0.52      0.54      0.53     37500\n",
            "weighted avg       0.52      0.54      0.53     37500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}